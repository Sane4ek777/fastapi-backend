import sqlite3
import requests
import openpyxl
from bs4 import BeautifulSoup
from fastapi import HTTPException, Body
from config import DB_FILE, XLSX_FILE
from logger import logger
import xml.etree.ElementTree as ET  # üëà –î–æ–±–∞–≤–∏–ª–∏ ET
from config import DB_FILE, XML_FILE  # üëà –î–æ–±–∞–≤–∏–ª–∏ XML_FILE
import re
import unicodedata

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

def generate_slug(name: str) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç slug –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–∞."""
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä –∏ –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É
    name = name.lower()  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä –±–µ–∑ —É–¥–∞–ª–µ–Ω–∏—è –∫–∏—Ä–∏–ª–ª–∏—Ü—ã

    # –£–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–∏–º–≤–æ–ª—ã, –∫—Ä–æ–º–µ –±—É–∫–≤, —Ü–∏—Ñ—Ä, –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –¥–µ—Ñ–∏—Å–æ–≤
    name = re.sub(r'[^a-zA-Z0-9\-–∞-—è–ê-–Ø—ë–Å\s]', '', name)  # –û—Å—Ç–∞–≤–ª—è–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É

    # –ó–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ –Ω–∏–∂–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è –Ω–∞ –¥–µ—Ñ–∏—Å—ã
    name = re.sub(r'[\s_-]+', '-', name).strip()

    return name


def generate_unique_slug(name, cursor):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π slug, –¥–æ–±–∞–≤–ª—è—è —Å—É—Ñ—Ñ–∏–∫—Å—ã –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏"""
    base_slug = generate_slug(name)
    slug = base_slug
    counter = 1

    cursor.execute("SELECT COUNT(*) FROM products WHERE slug = ?", (slug,))
    while cursor.fetchone()[0] > 0:  # –ü–æ–∫–∞ slug —É–∂–µ –µ—Å—Ç—å, –¥–æ–±–∞–≤–ª—è–µ–º —Å—É—Ñ—Ñ–∏–∫—Å
        slug = f"{base_slug}-{counter}"
        counter += 1
        cursor.execute("SELECT COUNT(*) FROM products WHERE slug = ?", (slug,))

    return slug

def get_price_from_xlsx(row_idx: int):
    """–ü–æ–ª—É—á–∞–µ—Ç —Ü–µ–Ω—É, –†–†–¶ –∏ –∞—Ä—Ç–∏–∫—É–ª –ø–æ –Ω–æ–º–µ—Ä—É —Å—Ç—Ä–æ–∫–∏ –≤ XLSX"""
    logger.info(f"Getting price info from XLSX for row {row_idx}")
    wb = openpyxl.load_workbook(XLSX_FILE)
    sheet = wb.active

    row = list(sheet.iter_rows(min_row=row_idx, max_row=row_idx))[0]
    price_cell = row[9]
    price_rrc_cell = row[10]
    article = row[1].value if len(row) >= 2 else None

    price = price_cell.value if price_cell else None
    price_rrc = price_rrc_cell.value if price_rrc_cell else None

    logger.info(f"Found price {price}, price_rrc {price_rrc}, article {article} at row {row_idx}")
    return price, price_rrc, article


def scrape_product_data(url: str, row_idx: int):
    """–°–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–≤–∞—Ä–µ, –≤–∫–ª—é—á–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
    try:
        logger.info(f"Scraping product data from URL: {url}, row: {row_idx}")
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            logger.error(f"Error {response.status_code} while requesting {url}")
            raise HTTPException(status_code=400, detail=f"–û—à–∏–±–∫–∞ {response.status_code} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}")

        soup = BeautifulSoup(response.text, 'html.parser')

        # –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞
        title_tag = soup.find('h2', class_='h2 hidden-tablet hidden-mobile')
        title = title_tag.get_text(strip=True) if title_tag else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'

        # –¶–µ–Ω–∞ –∏ –†–†–¶ –ø–æ –Ω–æ–º–µ—Ä—É —Å—Ç—Ä–æ–∫–∏
        price, price_rrc, article = get_price_from_xlsx(row_idx)

        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image_urls = []
        image_gallery = soup.find('div', class_='BigImagerGallery')
        if image_gallery:
            for img_tag in image_gallery.find_all('img'):
                src = img_tag.get('src')
                if src:
                    full_image_url = "https://www.diamir.su" + src if not src.startswith("http") else src
                    image_urls.append(full_image_url)

        # –û–ø–∏—Å–∞–Ω–∏–µ
        description_tag = soup.find('div', class_='cover-text')
        description = description_tag.get_text(separator=' ', strip=True) if description_tag else ''

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
        categories = extract_categories(soup)
        category_id = save_categories_to_db(categories)

        # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        attributes = scrape_attributes(soup, article)

        logger.info(f"Scraped data: {title}, Categories: {categories}, Price: {price}, Attributes: {attributes}")

        return {
            'name': title,
            'description': description,
            'price': price,
            'price_rrc': price_rrc,
            'images': image_urls,
            'category_id': category_id,
            'attributes': attributes
        }
    except HTTPException as http_error:
        logger.error(f"HTTP error occurred: {str(http_error)}")
        raise http_error
    except Exception as e:
        logger.error(f"Error occurred while scraping data from {url}: {str(e)}")
        return None


def scrape_all_products(urls_with_rows):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Ç–æ–≤–∞—Ä—ã –∏–∑ —Å–ø–∏—Å–∫–∞ URL –∏ –Ω–æ–º–µ—Ä–æ–≤ —Å—Ç—Ä–æ–∫
    urls_with_rows - —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (url, row_idx)
    """
    for url, row_idx in urls_with_rows:
        product_data = scrape_product_data(url, row_idx)
        if product_data:
            try:
                insert_scraped_product(product_data)
                logger.info(f"Successfully added product from URL: {url}")
            except Exception as e:
                logger.error(f"Error inserting product data from URL {url}: {str(e)}")
        else:
            logger.warning(f"Skipping product due to scraping error: {url}")


def scrape_attributes(soup, article: str):
    """–°–æ–±–∏—Ä–∞–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ç–æ–≤–∞—Ä–∞ —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    logger.info(f"Scraping attributes for article: {article}")
    attributes = []

    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –∞—Ä—Ç–∏–∫—É–ª –≤—Å–µ–≥–¥–∞ —Å—Ç—Ä–æ–∫–æ–≤—ã–π
    article = str(article).strip().zfill(6)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∞—Ä—Ç–∏–∫—É–ª –≤ —Å—Ç—Ä–æ–∫—É –∏ –¥–æ–±–∞–≤–ª—è–µ–º –≤–µ–¥—É—â–∏–µ –Ω—É–ª–∏

    table_section = soup.find('section', class_='info__table table js-table')
    if not table_section:
        logger.warning("Attributes table not found!")
        return attributes

    coll_blocks = table_section.find_all('div', class_='table__coll coll')

    article_position = None
    for block in coll_blocks:
        name_tag = block.find('div', class_='coll__name')
        if not name_tag:
            continue
        name_text = name_tag.get_text(strip=True)
        if "–ê—Ä—Ç–∏–∫—É–ª –¥–ª—è –∑–∞–∫–∞–∑–∞" in name_text:
            container = block.find('div', class_='coll__container')
            values = container.find_all('div')
            for idx, val_div in enumerate(values):
                val_text = val_div.get_text(strip=True)
                if val_text.zfill(6) == article:  # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∞—Ä—Ç–∏–∫—É–ª —Å –≤–µ–¥—É—â–∏–º–∏ –Ω—É–ª—è–º–∏
                    article_position = idx
                    logger.info(f"Found article {article} at position {article_position}")
                    break
            break

    if article_position is None:
        logger.warning(f"Article {article} not found in attributes table.")
        return attributes

    for block in coll_blocks:
        name_tag = block.find('div', class_='coll__name')
        if not name_tag:
            continue
        name_text = name_tag.get_text(strip=True)

        if "–ê—Ä—Ç–∏–∫—É–ª –¥–ª—è –∑–∞–∫–∞–∑–∞" in name_text:
            continue

        container = block.find('div', class_='coll__container')
        values = container.find_all('div')
        if len(values) > article_position:
            value_text = values[article_position].get_text(strip=True)
            attributes.append({'name': name_text, 'value': value_text})
            logger.info(f"Attribute parsed: {name_text} = {value_text}")

    return attributes

def generate_unique_product_id():
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è —Ç–æ–≤–∞—Ä–∞ –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞."""
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()

    try:
        while True:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π ID –¥–ª—è —Ç–æ–≤–∞—Ä–æ–≤
            cursor.execute("SELECT MAX(id) FROM products")
            max_id = cursor.fetchone()[0]
            
            if max_id is None:
                new_id = 1  # –ù–∞—á–∏–Ω–∞–µ–º —Å 1 –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞
            else:
                new_id = max_id + 1  # –°–ª–µ–¥—É—é—â–∏–π ID

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ç–æ–≤–∞—Ä —Å —Ç–∞–∫–∏–º ID
            cursor.execute("SELECT 1 FROM products WHERE id = ?", (new_id,))
            if cursor.fetchone() is None:
                # –ï—Å–ª–∏ —Ç–æ–≤–∞—Ä —Å —Ç–∞–∫–∏–º ID –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º ID
                conn.close()
                logger.info(f"Generated new product ID: {new_id}")
                return new_id

    except sqlite3.Error as e:
        logger.error(f"SQLite error: {str(e)}")
        conn.close()
        raise

    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        conn.close()
        raise

def insert_scraped_product(data):
    """–î–æ–±–∞–≤–ª—è–µ—Ç —Ç–æ–≤–∞—Ä –≤ –ë–î —Å —É—á–µ—Ç–æ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()

    try:
        new_id = generate_unique_product_id()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π slug
        slug = generate_unique_slug(data['name'], cursor)

        cursor.execute(
            "INSERT INTO products (id, name, slug, price, price_rrc, description, image, category_id, available) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)",
            (new_id, data['name'], slug, data['price'], data['price_rrc'], data['description'], ",".join(data['images']), data['category_id'], True)
        )

        if data.get('attributes'):
            for attr in data['attributes']:
                cursor.execute(
                    "INSERT INTO product_attributes (product_id, attribute_name, attribute_value) VALUES (?, ?, ?)",
                    (new_id, attr['name'], attr['value'])
                )

        conn.commit()
        logger.info(f"–¢–æ–≤–∞—Ä '{data['name']}' (slug: {slug}) —Å ID {new_id} —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.")
    except sqlite3.IntegrityError:
        logger.error(f"–û—à–∏–±–∫–∞: —É–Ω–∏–∫–∞–ª—å–Ω—ã–π slug —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü—Ä–æ–±–ª–µ–º–∞ —Å —Ç–æ–≤–∞—Ä–æ–º '{data['name']}'")
    except sqlite3.Error as e:
        logger.error(f"–û—à–∏–±–∫–∞ SQLite: {str(e)} –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ —Ç–æ–≤–∞—Ä–∞.")
    except Exception as e:
        logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)} –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ —Ç–æ–≤–∞—Ä–∞.")
    finally:
        conn.close()


def extract_categories(soup):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–æ–≤–∞—Ä–∞ –∏–∑ —Ö–ª–µ–±–Ω—ã—Ö –∫—Ä–æ—à–µ–∫"""
    breadcrumb_section = soup.find('section', class_='section breadcrumbs')
    if not breadcrumb_section:
        logger.warning("Breadcrumbs not found!")
        return [], []

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –≤ —Ö–ª–µ–±–Ω—ã—Ö –∫—Ä–æ—à–∫–∞—Ö
    breadcrumb_links = breadcrumb_section.find_all('a', class_='breadcrumbs__item')

    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º "–ì–ª–∞–≤–Ω–∞—è" –∏ "–ö–∞—Ç–∞–ª–æ–≥"
    if len(breadcrumb_links) < 4:
        logger.warning("Breadcrumbs structure is not as expected!")
        return breadcrumb_links[2:], []  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ 3-—é –ø–æ–∑–∏—Ü–∏—é –∫–∞–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏—é

    # 3-—è –ø–æ–∑–∏—Ü–∏—è - –∫–∞—Ç–µ–≥–æ—Ä–∏—è
    category = breadcrumb_links[2].get_text(strip=True)

    # –ï—Å–ª–∏ –µ—Å—Ç—å 4-–π —ç–ª–µ–º–µ–Ω—Ç, —ç—Ç–æ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è
    if len(breadcrumb_links) >= 5:
        subcategory = breadcrumb_links[3].get_text(strip=True)
    else:
        subcategory = None  # –ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç

    return category, subcategory

def save_categories_to_db(categories):
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ XML –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    tree = ET.parse(XML_FILE)
    root = tree.getroot()
    xml_categories = {}

    for category in root.findall(".//category"):
        cat_id = int(category.get("id"))
        name = category.text.strip() if category.text else ""
        xml_categories[name] = cat_id  # –°–ª–æ–≤–∞—Ä—å {–∏–º—è_–∫–∞—Ç–µ–≥–æ—Ä–∏–∏: id}

    parent_id = None  # ID —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏

    for category in categories:
        category_slug = generate_slug(category)  # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º slug –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–∞–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è –≤ XML
        if category in xml_categories:
            category_id = xml_categories[category]
        else:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ç–∞–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è –≤ –ë–î
            cursor.execute("SELECT id FROM categories WHERE name = ?", (category,))
            result = cursor.fetchone()

            if result:
                category_id = result[0]
            else:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å slug
                cursor.execute("SELECT COUNT(*) FROM categories WHERE slug = ?", (category_slug,))
                if cursor.fetchone()[0] > 0:
                    base_slug = category_slug
                    counter = 1
                    while True:
                        new_slug = f"{base_slug}-{counter}"
                        cursor.execute("SELECT COUNT(*) FROM categories WHERE slug = ?", (new_slug,))
                        if cursor.fetchone()[0] == 0:
                            category_slug = new_slug
                            break
                        counter += 1

                # –í—Å—Ç–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º slug
                try:
                    cursor.execute("INSERT INTO categories (name, slug, parent_id) VALUES (?, ?, ?)", 
                                   (category, category_slug, parent_id))
                    conn.commit()
                    category_id = cursor.lastrowid
                except sqlite3.IntegrityError:
                    logger.warning(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è {category} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ –ë–î.")
                    cursor.execute("SELECT id FROM categories WHERE name = ?", (category,))
                    category_id = cursor.fetchone()[0]

        parent_id = category_id  # –û–±–Ω–æ–≤–ª—è–µ–º parent_id –¥–ª—è –≤–ª–æ–∂–µ–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏

    conn.close()
    return parent_id  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º ID –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏

def is_url_valid(url: str) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –¥–æ—Å—Ç—É–ø–Ω–∞ –ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ URL (–∫–æ–¥ –æ—Ç–≤–µ—Ç–∞ –Ω–µ 404).
    """
    try:
        response = requests.head(url, allow_redirects=True, timeout=5)
        if response.status_code == 404:
            logger.warning(f"URL –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 404: {url}")
            return False
        logger.info(f"URL –¥–æ—Å—Ç—É–ø–µ–Ω: {url} (Status: {response.status_code})")
        return True
    except requests.RequestException as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ URL {url}: {str(e)}")
        return False

def parse_diamir_xlsx():
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã –∏–∑ XLSX-—Ñ–∞–π–ª–∞, –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∏—Ö –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∏ –≤—ã–∑—ã–≤–∞–µ—Ç –ø–∞—Ä—Å–∏–Ω–≥,
    –ø–µ—Ä–µ–¥–∞–≤–∞—è –Ω–æ–º–µ—Ä —Å—Ç—Ä–æ–∫–∏, –≤ –∫–æ—Ç–æ—Ä–æ–π –Ω–∞–π–¥–µ–Ω–∞ –≥–∏–ø–µ—Ä—Å—Å—ã–ª–∫–∞
    """
    logger.info("Parsing Diamir XLSX file with hyperlink extraction and inline scraping...")
    wb = openpyxl.load_workbook(XLSX_FILE)
    sheet = wb.active
    found_any_links = False  # –§–ª–∞–≥, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç, –±—ã–ª–∏ –ª–∏ –Ω–∞–π–¥–µ–Ω—ã —Å—Å—ã–ª–∫–∏

    for row_idx, row in enumerate(sheet.iter_rows(min_row=2), start=2):
        for cell in row:
            if cell.value == "–°—Å—ã–ª–∫–∞ –Ω–∞ —Å–∞–π—Ç" and cell.hyperlink:
                url = cell.hyperlink.target
                logger.info(f"Found hyperlink: {url} at row {row_idx}")
                found_any_links = True

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å URL –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º
                if is_url_valid(url):
                    scrape_all_products([(url, row_idx)])
                else:
                    logger.warning(f"–ü—Ä–æ–ø—É—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ–π —Å—Å—ã–ª–∫–∏: {url}")

    if not found_any_links:
        logger.warning("No links found in XLSX file.")

    logger.info("Finished parsing and scraping XLSX file.")