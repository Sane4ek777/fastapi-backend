import sqlite3
import requests
import openpyxl
from bs4 import BeautifulSoup
from fastapi import HTTPException, Body
from config import DB_FILE, XLSX_FILE
from logger import logger
import xml.etree.ElementTree as ET  # üëà –î–æ–±–∞–≤–∏–ª–∏ ET
from config import DB_FILE, XML_FILE  # üëà –î–æ–±–∞–≤–∏–ª–∏ XML_FILE
import re
import unicodedata
from transliterate import translit

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

def generate_slug(name: str) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç slug –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–∞, –∑–∞–º–µ–Ω—è—è –∫–∏—Ä–∏–ª–ª–∏—Ü—É –Ω–∞ –ª–∞—Ç–∏–Ω–∏—Ü—É."""
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä
    name = name.lower()

    # –¢—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∏—Ä—É–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É –≤ –ª–∞—Ç–∏–Ω–∏—Ü—É
    name = translit(name, 'ru', reversed=True)  # 'ru' - –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞, reversed=True –¥–ª—è –∫–∏—Ä–∏–ª–ª–∏—Ü—ã –≤ –ª–∞—Ç–∏–Ω–∏—Ü—É

    # –£–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–∏–º–≤–æ–ª—ã, –∫—Ä–æ–º–µ –±—É–∫–≤, —Ü–∏—Ñ—Ä, –¥–µ—Ñ–∏—Å–æ–≤ –∏ –ø—Ä–æ–±–µ–ª–æ–≤
    name = re.sub(r'[^a-zA-Z0-9\s-]', '', name)  # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ª–∞—Ç–∏–Ω–∏—Ü—É, —Ü–∏—Ñ—Ä—ã, –ø—Ä–æ–±–µ–ª—ã –∏ –¥–µ—Ñ–∏—Å—ã

    # –ó–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ –Ω–∏–∂–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è –Ω–∞ –¥–µ—Ñ–∏—Å—ã
    name = re.sub(r'[\s_-]+', '-', name).strip()  # –ó–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è –Ω–∞ –¥–µ—Ñ–∏—Å—ã

    return name


def generate_unique_slug(name, cursor):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π slug, –¥–æ–±–∞–≤–ª—è—è —Å—É—Ñ—Ñ–∏–∫—Å—ã –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏"""
    base_slug = generate_slug(name)
    slug = base_slug
    counter = 1

    cursor.execute("SELECT COUNT(*) FROM products WHERE slug = ?", (slug,))
    while cursor.fetchone()[0] > 0:  # –ü–æ–∫–∞ slug —É–∂–µ –µ—Å—Ç—å, –¥–æ–±–∞–≤–ª—è–µ–º —Å—É—Ñ—Ñ–∏–∫—Å
        slug = f"{base_slug}-{counter}"
        counter += 1
        cursor.execute("SELECT COUNT(*) FROM products WHERE slug = ?", (slug,))

    return slug

def get_price_from_xlsx(row_idx: int):
    """–ü–æ–ª—É—á–∞–µ—Ç —Ü–µ–Ω—É, –†–†–¶ –∏ –∞—Ä—Ç–∏–∫—É–ª –ø–æ –Ω–æ–º–µ—Ä—É —Å—Ç—Ä–æ–∫–∏ –≤ XLSX"""
    logger.info(f"Getting price info from XLSX for row {row_idx}")
    wb = openpyxl.load_workbook(XLSX_FILE)
    sheet = wb.active

    row = list(sheet.iter_rows(min_row=row_idx, max_row=row_idx))[0]
    price_cell = row[9]
    price_rrc_cell = row[10]
    article = row[1].value if len(row) >= 2 else None

    price = price_cell.value if price_cell else None
    price_rrc = price_rrc_cell.value if price_rrc_cell else None

    logger.info(f"Found price {price}, price_rrc {price_rrc}, article {article} at row {row_idx}")
    return price, price_rrc, article

def scrape_product_data(url: str, row_idx: int):
    """–°–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–≤–∞—Ä–µ, –≤–∫–ª—é—á–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
    try:
        logger.info(f"Scraping product data from URL: {url}, row: {row_idx}")
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            logger.error(f"Error {response.status_code} while requesting {url}")
            raise HTTPException(status_code=400, detail=f"–û—à–∏–±–∫–∞ {response.status_code} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}")

        soup = BeautifulSoup(response.text, 'html.parser')

        # –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞
        title_tag = soup.find('h2', class_='h2 hidden-tablet hidden-mobile')
        title = title_tag.get_text(strip=True) if title_tag else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'

        # –¶–µ–Ω–∞ –∏ –†–†–¶ –ø–æ –Ω–æ–º–µ—Ä—É —Å—Ç—Ä–æ–∫–∏
        price, price_rrc, article = get_price_from_xlsx(row_idx)

        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image_urls = []
        image_gallery = soup.find('div', class_='BigImagerGallery')
        if image_gallery:
            for img_tag in image_gallery.find_all('img'):
                src = img_tag.get('src')
                if src:
                    full_image_url = "https://www.diamir.su" + src if not src.startswith("http") else src
                    image_urls.append(full_image_url)

        # –û–ø–∏—Å–∞–Ω–∏–µ
        description_tag = soup.find('div', class_='cover-text')
        description = description_tag.get_text(separator=' ', strip=True) if description_tag else ''

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
        categories = extract_categories(soup)
        category_id = save_categories_to_db(categories)

        # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        attributes = scrape_attributes(soup, article)
        if not attributes:
            logger.warning(f"No attributes found for article {article}. Skipping product.")
            return None
        logger.info(f"Scraped data: {title}, Categories: {categories}, Price: {price}, Attributes: {attributes}")

        return {
            'name': title,
            'description': description,
            'price': price,
            'price_rrc': price_rrc,
            'images': image_urls,
            'category_id': category_id,
            'attributes': attributes
        }
    except HTTPException as http_error:
        logger.error(f"HTTP error occurred: {str(http_error)}")
        raise http_error
    except Exception as e:
        logger.error(f"Error occurred while scraping data from {url}: {str(e)}")
        return None

def update_product_names():
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()

    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–≤–∞—Ä—ã –∏–∑ —Ñ–∞–π–ª–∞ Diamir
    cursor.execute("SELECT id, name FROM products WHERE is_diamir = 1")
    products = cursor.fetchall()

    for product_id, name in products:
        cursor.execute("SELECT attribute_name, attribute_value FROM product_attributes WHERE product_id = ?", (product_id,))
        attributes = cursor.fetchall()

        attribute_counts = {}
        attr_priority = {"–∑–µ—Ä–Ω": None, "–¥–∏–∞–º–µ—Ç—Ä": None}

        for attr_name, attr_value in attributes:
            if attr_name == "–ê—Ä—Ç–∏–∫—É–ª –¥–ª—è –∑–∞–∫–∞–∑–∞":
                continue

            cleaned_value = attr_value.strip()

            # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –ø–µ—Ä–≤—É—é –ø–æ–ø–∞–≤—à—É—é—Å—è –ø–æ–¥—Ö–æ–¥—è—â—É—é —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
            lower_name = attr_name.lower()
            if "–∑–µ—Ä–Ω" in lower_name and attr_priority["–∑–µ—Ä–Ω"] is None:
                attr_priority["–∑–µ—Ä–Ω"] = (attr_name, cleaned_value)
            elif "–¥–∏–∞–º–µ—Ç—Ä" in lower_name and attr_priority["–¥–∏–∞–º–µ—Ç—Ä"] is None:
                attr_priority["–¥–∏–∞–º–µ—Ç—Ä"] = (attr_name, cleaned_value)

            if attr_name not in attribute_counts:
                attribute_counts[attr_name] = set()
            attribute_counts[attr_name].add(cleaned_value)

        # –í—ã–±–∏—Ä–∞–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        significant_attr = None
        if attr_priority["–∑–µ—Ä–Ω"]:
            significant_attr = attr_priority["–∑–µ—Ä–Ω"]
        elif attr_priority["–¥–∏–∞–º–µ—Ç—Ä"]:
            significant_attr = attr_priority["–¥–∏–∞–º–µ—Ç—Ä"]
        else:
            max_unique_values = 0
            for attr_name, values in attribute_counts.items():
                if len(values) > max_unique_values:
                    max_unique_values = len(values)
                    significant_attr = (attr_name, next(iter(values)))

        # –û–±–Ω–æ–≤–ª—è–µ–º –∏–º—è, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if significant_attr:
            attr_str = f"({significant_attr[0]}: {significant_attr[1]})"
            if attr_str not in name:
                new_name = f"{name} {attr_str}"
                cursor.execute("UPDATE products SET name = ? WHERE id = ?", (new_name, product_id))

    conn.commit()
    conn.close()
    
def scrape_all_products(urls_with_rows):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Ç–æ–≤–∞—Ä—ã –∏–∑ —Å–ø–∏—Å–∫–∞ URL –∏ –Ω–æ–º–µ—Ä–æ–≤ —Å—Ç—Ä–æ–∫
    urls_with_rows - —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (url, row_idx)
    """
    for url, row_idx in urls_with_rows:
        product_data = scrape_product_data(url, row_idx)
        if product_data:
            try:
                insert_scraped_product(product_data)
                logger.info(f"Successfully added product from URL: {url}")
            except Exception as e:
                logger.error(f"Error inserting product data from URL {url}: {str(e)}")
        else:
            logger.warning(f"Skipping product due to scraping error: {url}")
    update_product_names()

def scrape_attributes(soup, article: str):
    """–°–æ–±–∏—Ä–∞–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ç–æ–≤–∞—Ä–∞ —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    logger.info(f"Scraping attributes for article: {article}")
    attributes = []

    article = str(article).strip().zfill(6)

    table_section = soup.find('section', class_='info__table table js-table')
    if not table_section:
        logger.warning("Attributes table not found!")
        return attributes

    coll_blocks = table_section.find_all('div', class_='table__coll coll')

    # –ù–∞–π–¥—ë–º –±–ª–æ–∫ —Å –∞—Ä—Ç–∏–∫—É–ª–∞–º–∏ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏–º, —Å–∫–æ–ª—å–∫–æ –∏—Ö
    article_position = None
    article_values_count = 0
    for block in coll_blocks:
        name_tag = block.find('div', class_='coll__name')
        if name_tag and "–ê—Ä—Ç–∏–∫—É–ª –¥–ª—è –∑–∞–∫–∞–∑–∞" in name_tag.get_text(strip=True):
            container = block.find('div', class_='coll__container')
            value_divs = container.find_all('div', recursive=False)
            article_values_count = len(value_divs)

            # –ï—Å–ª–∏ –±–æ–ª—å—à–µ –æ–¥–Ω–æ–≥–æ –∞—Ä—Ç–∏–∫—É–ª–∞ ‚Äî –æ–ø—Ä–µ–¥–µ–ª–∏–º –ø–æ–∑–∏—Ü–∏—é
            if article_values_count > 1:
                for idx, val_div in enumerate(value_divs):
                    val_text = val_div.get_text(strip=True)
                    if val_text.zfill(6) == article:
                        article_position = idx
                        logger.info(f"Found article {article} at position {article_position}")
                        break
            break

    if article_values_count > 1 and article_position is None:
        logger.warning(f"Article {article} not found in attributes table.")
        return attributes

    # –°–±–æ—Ä —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
    if article_values_count > 1:
        # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞—Ä—Ç–∏–∫—É–ª—ã ‚Äî —Å–æ–±–∏—Ä–∞–µ–º –ø–æ –ø–æ–∑–∏—Ü–∏–∏
        for block in coll_blocks:
            name_tag = block.find('div', class_='coll__name')
            if not name_tag:
                continue
            name_text = name_tag.get_text(strip=True)
            container = block.find('div', class_='coll__container')
            value_divs = container.find_all('div', recursive=False)

            if len(value_divs) > article_position:
                value_text = value_divs[article_position].get_text(strip=True)
                attributes.append({'name': name_text, 'value': value_text})
                logger.info(f"Attribute parsed: {name_text} = {value_text}")

    else:
        # –û–¥–∏–Ω –∞—Ä—Ç–∏–∫—É–ª ‚Äî –Ω–∞–∑–≤–∞–Ω–∏—è –≤ –ø–µ—Ä–≤–æ–º –±–ª–æ–∫–µ, –∑–Ω–∞—á–µ–Ω–∏—è –≤–æ –≤—Ç–æ—Ä–æ–º
        if len(coll_blocks) < 2:
            logger.warning("Expected two columns for single article, but found less.")
            return attributes

        # –ù–∞–∑–≤–∞–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        name_block = coll_blocks[0]
        name_container = name_block.find('div', class_='coll__container')
        name_divs = name_container.find_all('div', recursive=False)

        # –ó–Ω–∞—á–µ–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        value_block = coll_blocks[1]
        value_container = value_block.find('div', class_='coll__container')
        value_divs = value_container.find_all('div', recursive=False)

        # –°–æ–ø–æ—Å—Ç–∞–≤–∏–º –Ω–∞–∑–≤–∞–Ω–∏—è –∏ –∑–Ω–∞—á–µ–Ω–∏—è
        for name_div, value_div in zip(name_divs, value_divs):
            name_text = name_div.get_text(strip=True)
            value_text = value_div.get_text(strip=True)
            attributes.append({'name': name_text, 'value': value_text})
            logger.info(f"Attribute parsed (single article): {name_text} = {value_text}")

    return attributes

def generate_unique_product_id():
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è —Ç–æ–≤–∞—Ä–∞ –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞."""
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()

    try:
        while True:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π ID –¥–ª—è —Ç–æ–≤–∞—Ä–æ–≤
            cursor.execute("SELECT MAX(id) FROM products")
            max_id = cursor.fetchone()[0]
            
            if max_id is None:
                new_id = 1  # –ù–∞—á–∏–Ω–∞–µ–º —Å 1 –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞
            else:
                new_id = max_id + 1  # –°–ª–µ–¥—É—é—â–∏–π ID

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ç–æ–≤–∞—Ä —Å —Ç–∞–∫–∏–º ID
            cursor.execute("SELECT 1 FROM products WHERE id = ?", (new_id,))
            if cursor.fetchone() is None:
                # –ï—Å–ª–∏ —Ç–æ–≤–∞—Ä —Å —Ç–∞–∫–∏–º ID –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º ID
                conn.close()
                logger.info(f"Generated new product ID: {new_id}")
                return new_id

    except sqlite3.Error as e:
        logger.error(f"SQLite error: {str(e)}")
        conn.close()
        raise

    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        conn.close()
        raise

def insert_scraped_product(data):
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()

    try:
        # 1. –ò–∑–≤–ª–µ—á—å –∞—Ä—Ç–∏–∫—É–ª
        article = None
        for attr in data.get('attributes', []):
            if attr['name'].strip().lower() == "–∞—Ä—Ç–∏–∫—É–ª –¥–ª—è –∑–∞–∫–∞–∑–∞":
                article = attr['value'].strip()
                break

        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞
        if article:
            cursor.execute("""
                SELECT p.id FROM products p
                JOIN product_attributes a ON a.product_id = p.id
                WHERE a.attribute_name = '–ê—Ä—Ç–∏–∫—É–ª –¥–ª—è –∑–∞–∫–∞–∑–∞' AND a.attribute_value = ?
            """, (article,))
            
            existing = cursor.fetchone()
            if existing:
                logger.warning(f"–¢–æ–≤–∞—Ä —Å –∞—Ä—Ç–∏–∫—É–ª–æ–º {article} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç (ID: {existing[0]}), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")
                conn.close()
                return

        # 3. –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ
        new_id = generate_unique_product_id()
        slug = generate_unique_slug(data['name'], cursor)

        cursor.execute(
            "INSERT INTO products (id, name, slug, price, price_rrc, description, image, category_id, available, is_diamir) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
            (new_id, data['name'], slug, data['price'], data['price_rrc'], data['description'], ",".join(data['images']), data['category_id'], True, True)
        )

        for attr in data.get('attributes', []):
            cursor.execute(
                "INSERT INTO product_attributes (product_id, attribute_name, attribute_value) VALUES (?, ?, ?)",
                (new_id, attr['name'], attr['value'])
            )

        conn.commit()
        logger.info(f"–¢–æ–≤–∞—Ä '{data['name']}' (slug: {slug}) —Å ID {new_id} —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω.")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ —Ç–æ–≤–∞—Ä–∞: {str(e)}")
    finally:
        conn.close()

def normalize_name(name):
    """–ü—Ä–∏–≤–æ–¥–∏—Ç –∏–º—è –∫ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É –≤–∏–¥—É –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    return name.strip().lower()

def extract_categories(soup):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—é –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ —Ö–ª–µ–±–Ω—ã—Ö –∫—Ä–æ—à–µ–∫, –∏—Å–∫–ª—é—á–∞—è '–ì–ª–∞–≤–Ω–∞—è', '–ö–∞—Ç–∞–ª–æ–≥' –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞"""
    breadcrumb_section = soup.find('section', class_='section breadcrumbs')
    if not breadcrumb_section:
        logger.warning("Breadcrumbs not found!")
        return []

    categories = []

    for link in breadcrumb_section.find_all('a', class_='breadcrumbs__item'):
        text = link.get_text(strip=True)
        href = link.get('href')

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º "–ì–ª–∞–≤–Ω–∞—è" –∏ "–ö–∞—Ç–∞–ª–æ–≥"
        if text in ('–ì–ª–∞–≤–Ω–∞—è', '–ö–∞—Ç–∞–ª–æ–≥'):
            continue

        # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å href
        if href:
            categories.append(text)

    return categories

def save_categories_to_db(categories):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ –ë–î —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∏–µ—Ä–∞—Ä—Ö–∏–µ–π parent_id"""
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ XML
    tree = ET.parse(XML_FILE)
    root = tree.getroot()
    xml_categories = {
        normalize_name(category.text): int(category.get("id"))
        for category in root.findall(".//category") if category.text
    }

    parent_id = None  # ID —Ä–æ–¥–∏—Ç–µ–ª—è –¥–ª—è —Ç–µ–∫—É—â–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏

    for category in categories:
        normalized = normalize_name(category)
        category_slug = generate_slug(category)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤ XML
        if normalized in xml_categories:
            category_id = xml_categories[normalized]
        else:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤ –ë–î –ø–æ –∏–º–µ–Ω–∏
            cursor.execute("SELECT id FROM categories WHERE name = ?", (category,))
            result = cursor.fetchone()

            if result:
                category_id = result[0]
            else:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ slug
                cursor.execute("SELECT COUNT(*) FROM categories WHERE slug = ?", (category_slug,))
                if cursor.fetchone()[0] > 0:
                    base_slug = category_slug
                    counter = 1
                    while True:
                        new_slug = f"{base_slug}-{counter}"
                        cursor.execute("SELECT COUNT(*) FROM categories WHERE slug = ?", (new_slug,))
                        if cursor.fetchone()[0] == 0:
                            category_slug = new_slug
                            break
                        counter += 1

                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è ID (–º–æ–∂–Ω–æ –ø—Ä–æ—Å—Ç–æ –æ—Å—Ç–∞–≤–∏—Ç—å –∞–≤—Ç–æ–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç, –Ω–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–¥—Ö–æ–¥)
                cursor.execute("SELECT MAX(id) FROM categories")
                max_id = cursor.fetchone()[0] or 0
                new_category_id = max_id + 1

                try:
                    cursor.execute(
                        "INSERT INTO categories (id, name, slug, parent_id) VALUES (?, ?, ?, ?)",
                        (new_category_id, category, category_slug, parent_id)
                    )
                    conn.commit()
                    category_id = new_category_id
                    logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—è '{category}' —Å id={category_id} –∏ parent_id={parent_id}")
                except sqlite3.IntegrityError:
                    logger.warning(f"ID {new_category_id} —É–∂–µ –∑–∞–Ω—è—Ç, –≤—Å—Ç–∞–≤–ª—è–µ–º –±–µ–∑ ID")
                    cursor.execute(
                        "INSERT INTO categories (name, slug, parent_id) VALUES (?, ?, ?)",
                        (category, category_slug, parent_id)
                    )
                    conn.commit()
                    category_id = cursor.lastrowid
                    logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—è '{category}' (autoid) —Å parent_id={parent_id}")

        parent_id = category_id  # –ù–∞–∑–Ω–∞—á–∞–µ–º —Å–ª–µ–¥—É—é—â–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —ç—Ç–æ–≥–æ –∫–∞–∫ —Ä–æ–¥–∏—Ç–µ–ª—è

    conn.close()
    return parent_id  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º ID –ø–æ—Å–ª–µ–¥–Ω–µ–π (–≥–ª—É–±–æ–∫–æ–π) –∫–∞—Ç–µ–≥–æ—Ä–∏–∏

def is_url_valid(url: str) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –¥–æ—Å—Ç—É–ø–Ω–∞ –ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ URL (–∫–æ–¥ –æ—Ç–≤–µ—Ç–∞ –Ω–µ 404).
    """
    try:
        response = requests.head(url, allow_redirects=True, timeout=5)
        if response.status_code == 404:
            logger.warning(f"URL –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 404: {url}")
            return False
        logger.info(f"URL –¥–æ—Å—Ç—É–ø–µ–Ω: {url} (Status: {response.status_code})")
        return True
    except requests.RequestException as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ URL {url}: {str(e)}")
        return False

def parse_diamir_xlsx():
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã –∏–∑ XLSX-—Ñ–∞–π–ª–∞, –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∏—Ö –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∏ –≤—ã–∑—ã–≤–∞–µ—Ç –ø–∞—Ä—Å–∏–Ω–≥,
    –ø–µ—Ä–µ–¥–∞–≤–∞—è –Ω–æ–º–µ—Ä —Å—Ç—Ä–æ–∫–∏, –≤ –∫–æ—Ç–æ—Ä–æ–π –Ω–∞–π–¥–µ–Ω–∞ –≥–∏–ø–µ—Ä—Å—Å—ã–ª–∫–∞
    """
    logger.info("Parsing Diamir XLSX file with hyperlink extraction and inline scraping...")
    wb = openpyxl.load_workbook(XLSX_FILE)
    sheet = wb.active
    found_any_links = False  # –§–ª–∞–≥, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç, –±—ã–ª–∏ –ª–∏ –Ω–∞–π–¥–µ–Ω—ã —Å—Å—ã–ª–∫–∏

    for row_idx, row in enumerate(sheet.iter_rows(min_row=2), start=2):
        for cell in row:
            if cell.value == "–°—Å—ã–ª–∫–∞ –Ω–∞ —Å–∞–π—Ç" and cell.hyperlink:
                url = cell.hyperlink.target
                logger.info(f"Found hyperlink: {url} at row {row_idx}")
                found_any_links = True

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å URL –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º
                if is_url_valid(url):
                    scrape_all_products([(url, row_idx)])
                else:
                    logger.warning(f"–ü—Ä–æ–ø—É—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ–π —Å—Å—ã–ª–∫–∏: {url}")

    if not found_any_links:
        logger.warning("No links found in XLSX file.")

    logger.info("Finished parsing and scraping XLSX file.")

    